{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Food100 Classification using MobileNet V2"},{"metadata":{},"cell_type":"markdown","source":"## Dataset : [UEC Food-100](http://foodcam.mobi/dataset100.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir('../input/uecfood100'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/uecfood100/UECFOOD100'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dirNames = os.listdir(train_dir)\ndirNames.remove('category_ja_sjis.txt')\n\nfor dirName in dirNames:\n    if '.txt' in dirName:\n        print(dirName)\n        dirNames.remove(dirName)\n\n\nprint(dirNames)\nprint(len(dirNames))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### get labels from category.txt"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(train_dir+'/category.txt', sep='\\t')\nprint(df)\n\nlabels = df['name'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dirNames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainFiles = []\ntrainClasses = []\n\nfor dirName in dirNames:\n    for file in os.listdir(train_dir+\"/\"+dirName):\n        trainFiles.append(train_dir+\"/\"+dirName+\"/\"+file)\n        trainClasses.append(dirName)\n\nprint(len(trainFiles), len(trainClasses))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### display image of a training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(mpimg.imread(trainFiles[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Equilibre "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\ndef plot_equilibre(categories, counts):\n\n    plt.figure(figsize=(12, 8))\n\n    sns_bar = sns.barplot(x=categories, y=counts)\n    sns_bar.set_xticklabels(categories, rotation=45)\n    plt.title('Equilibre of Training Dataset')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = dirNames\ncounts = []\n[counts.append(trainClasses.count(dirName)) for dirName in dirNames]\n\nplot_equilibre(categories, counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.utils import to_categorical\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_size=(224,224)\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    vertical_flip=True)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    color_mode='rgb',    \n    shuffle=True,\n    seed=42,\n    class_mode='categorical')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.models import Model, save_model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, LeakyReLU, Concatenate\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 100\ninput_shape = (224,224,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build Model\nnet = MobileNetV2(input_shape=input_shape, weights='imagenet', include_top=False)\n\n# add two FC layers (with L2 regularization)\nx = net.output\nx = GlobalAveragePooling2D()(x) \n#x = BatchNormalization()(x)\n\n#x = Dense(256, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\nx = Dense(1024)(x)\n#x = Dropout(0.2)(x)\n\n#x = Dense(256, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\nx = Dense(256)(x)\n#x = Dropout(0.2)(x)\n\n# Output layer\nout = Dense(num_classes, activation=\"softmax\")(x)\n\nmodel = Model(inputs=net.input, outputs=out)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transfer Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile Model\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## set Checkpoint : save best only, verbose on\n#checkpoint = ModelCheckpoint(\"food100_mobilenetv2.hdf5\", monitor='accuracy', verbose=0, save_best_only=True, mode='auto', save_freq=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\nnum_epochs = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Model\nhistory = model.fit_generator(train_generator,steps_per_epoch=STEP_SIZE_TRAIN,epochs=num_epochs) #, callbacks=[checkpoint])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Save Model\nsave_model(model, 'food100_mobilenetv2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load best model weights if using callback (save-best-only)\n#model.load_weights(\"food100_mobilenetv2.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#score = model.evaluate_generator(test_generator, steps=STEP_SIZE_TEST)\n#print(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#predY=model.predict_generator(test_generator)\n#y_pred = np.argmax(predY,axis=1)\n##y_label= [labels[k] for k in y_pred]\n#y_actual = test_generator.classes\n#cm = confusion_matrix(y_actual, y_pred)\n#print(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### report confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(classification_report(y_actual, y_pred, target_names=labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Model"},{"metadata":{},"cell_type":"markdown","source":"### try one image"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_name  = '1'\nfile_name = '1.jpg'\ntestfile  = train_dir+'/'+dir_name+'/'+file_name\nplt.imshow(mpimg.imread(testfile))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\ndef prepare_image(filepath):\n    img = Image.open(filepath)  \n    out = img.resize((224, 224)) # (width, height), resample\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = prepare_image(testfile)\ntestData = np.array(img).reshape(1,224,224,3)\ntestData = testData / 255.0\npredictions = model.predict(testData)\nprint(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxindex = int(np.argmax(predictions[0]))\nprint('Predicted: %s, Probability = %f' %(labels[maxindex], predictions[0][maxindex]) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}