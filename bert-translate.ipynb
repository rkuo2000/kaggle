{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BERT Translate (English to Chinese)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pytorch_pretrained_bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering, BertForPreTraining ,BertAdam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_text = \"[CLS] I go to school by bus [SEP] \"\ntarget_text = \"我搭公車上學\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' # 'cpu'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load pretrained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre-trained model tokenizer (vocabulary)\nmodelpath = \"bert-base-chinese\"\ntokenizer = BertTokenizer.from_pretrained(modelpath)\nmodel = BertForMaskedLM.from_pretrained(modelpath)\nmodel.to(device)\n\ntokenized_text = tokenizer.tokenize(input_text)\nfor i in target_text:\n  tokenized_text.append('[MASK]')\n# tokenized_text.append('[SEP]')\nfor _ in range(128-len(tokenized_text)):\n  tokenized_text.append('[MASK]')\n# tokenized_text.append('[MASK]')\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n\ntokens_tensor = torch.tensor([indexed_tokens]).to(device)\n\nloss_ids = []\nloss_ids = [-1] * (len(tokenizer.tokenize(input_text)))\n# loss_ids.extend(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(input_text)))\nfor i in target_text:\n  loss_ids.append(tokenizer.convert_tokens_to_ids(i)[0])\nloss_ids.append(tokenizer.convert_tokens_to_ids(['[SEP]'])[0])\nfor _ in range(128-len(loss_ids)):\n  loss_ids.append(-1)\nloss_tensors = torch.tensor([loss_ids]).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokens_tensor,loss_tensors)\nprint(tokenizer.convert_ids_to_tokens(indexed_tokens))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_optimizer = list(model.named_parameters())\n\n# # hack to remove pooler, which is not used\n# # thus it produce None grad that break apex\n# param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n\n# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n# optimizer_grouped_parameters = [\n#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.05},\n#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n#         ]\n# optimizer = BertAdam(optimizer_grouped_parameters,\n#                              lr=5e-5)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=5e-7)\n# optimizer = torch.optim.SGD(model.parameters(), lr = 5e-5, momentum=0.9)\noptimizer = torch.optim.Adamax(model.parameters(), lr = 5e-5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\nfor i in range(0,300):\n  loss = model(tokens_tensor,masked_lm_labels=loss_tensors)\n  eveloss = loss.mean().item()\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n  print(\"step \"+ str(i) + \" : \" + str(eveloss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n  predictions = model(tokens_tensor)\n  start = len(tokenizer.tokenize(input_text))\n  while start < len(predictions[0]):\n    predicted_index = torch.argmax(predictions[0,start]).item()\n    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n    if '[SEP]' in predicted_token:\n        break\n    print(predicted_token)\n    start+=1","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}